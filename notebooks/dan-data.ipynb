{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configparser import ConfigParser\n",
    "import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import pkgutil\n",
    "import sys\n",
    "from typing import Optional, Union\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import arcpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_prj = Path.cwd().parent\n",
    "\n",
    "in_dir = dir_prj / 'data' / 'ad_hoc' / 'export20231003'\n",
    "in_pqt = in_dir / 'parquet'\n",
    "\n",
    "out_dir = dir_prj / 'data' / 'processed' / 'dan20231003'\n",
    "output_fgdb = out_dir / 'foursquare.gdb'\n",
    "out_fc = output_fgdb / 'places'\n",
    "zip_pth = out_dir / 'foursquare_geoenrichment.zip'\n",
    "\n",
    "src_dir = dir_prj / 'src'\n",
    "\n",
    "sys.path.insert(0, str(src_dir))\n",
    "\n",
    "from arcpy_parquet import parquet_to_feature_class \n",
    "\n",
    "if not out_dir.exists():\n",
    "    out_dir.mkdir()\n",
    "\n",
    "assert in_dir.exists()\n",
    "assert out_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('dan20231003')\n",
    "\n",
    "timestamp_str = datetime.datetime.today().strftime('%Y%m%d')\n",
    "fh = logging.FileHandler(str(out_dir / f\"dan20231003_{timestamp_str}.log\"))\n",
    "fh.setFormatter(logging.Formatter('%(asctime)s | %(name)s | %(levelname)s | %(message)s'))\n",
    "logger.addHandler(fh)\n",
    "\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.propagate = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flush the geodatabase to avoid any corruption issues\n",
    "if arcpy.Exists(str(output_fgdb)):\n",
    "    arcpy.management.Delete(str(output_fgdb))\n",
    "\n",
    "# create the output file geodatabase\n",
    "logger.info(f'Starting creation of File Geodatabase - {output_fgdb}')\n",
    "arcpy.management.CreateFileGDB(str(output_fgdb.parent), str(output_fgdb.stem))\n",
    "logger.info(f'Finished creation of File Geodatabase.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/projects/foursquare-data-loading/data/ad_hoc/export20231003/schema/part-00000-9e7cf37a-7f77-4e1d-a4ee-a1e217b67942-c000.csv')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_schema_csv(schema_dir: Union[Path, str]) -> Path:\n",
    "    \"\"\"Helper function to retrieve the csv for the schema when saved as a single part file from Spark.\"\"\"\n",
    "    # ensure we are working with a Path object\n",
    "    if isinstance(schema_dir, str):\n",
    "        schema_dir = Path(schema_dir)\n",
    "\n",
    "    # if working in a directory\n",
    "    if schema_dir.is_dir():\n",
    "\n",
    "        # get part csv file\n",
    "        prt_lst = [fl for fl in schema_dir.glob('part-*.csv')]\n",
    "\n",
    "        # ensure there even is a part file to work with\n",
    "        if len(prt_lst) == 0:\n",
    "            raise ValueError('Cannot locate a part*.csv file in the directory tree.')\n",
    "        else:\n",
    "            schema_csv = prt_lst[0]\n",
    "\n",
    "    # if just the file was passed\n",
    "    elif schema_dir.suffix == '.csv':\n",
    "        schema_csv = schema_dir\n",
    "\n",
    "    # pitch a fit if cannot figure out what to  do\n",
    "    else:\n",
    "        raise ValueError('Cannot locate a schema *.csv file.')\n",
    "    \n",
    "    return schema_csv\n",
    "\n",
    "schema_csv = get_schema_csv(in_dir / 'schema')\n",
    "\n",
    "schema_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/projects/foursquare-data-loading/data/processed/dan20231003/foursquare.gdb/places')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the conversion\n",
    "logger.info('Starting parquet data import.')\n",
    "\n",
    "# convert to feature class\n",
    "parquet_to_feature_class(\n",
    "    parquet_path=in_pqt,\n",
    "    output_feature_class=out_fc,\n",
    "    schema_file=schema_csv,\n",
    "    spatial_reference=3857,\n",
    "    build_spatial_index=True,\n",
    "    logger=logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compress the file geodatabase\n",
    "with ZipFile(zip_pth, mode='w', compresslevel=9) as zipper:\n",
    "\n",
    "    # iterate the files comprising the file geodatabase and add to the archive\n",
    "    for gdb_file in out_dir.glob('**/*.gdb/**/*'):\n",
    "        target_pth = str(gdb_file.relative_to(out_dir))\n",
    "        zipper.write(gdb_file, target_pth)\n",
    "\n",
    "logger.info(f'Successfully created archive {zip_pth}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
